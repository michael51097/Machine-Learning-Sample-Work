{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wifsxsI6YUvX"
   },
   "source": [
    "# Assignment 4: PCA and Clustering (due midnight 2/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfs-EuyJYUvY"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q49Z8m6gYUvb"
   },
   "source": [
    "## Part 1: Parsing data\n",
    "\n",
    "Youâ€™re given a dataset of gene expression for 20 samples that might represent different biological situations. \n",
    "\n",
    "- Read in the data as a matrix of n rows (number of samples) by m (number of genes measured for each sample). **Each sample (row) is a data point, and each gene (column) is a feature**.\n",
    "\n",
    "- Normalize the data: calculate the mean expression for each gene, and rescale each individual measurement relative to that mean (giving the fold change relative to the mean). \n",
    "\n",
    "- Then, log_2 transform the data.\n",
    "\n",
    "- Zero mean the data - subtract each column's mean from each value in that column\n",
    "\n",
    "<font color=red>**Add code cells below this cell in order to parse the data.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YpXsiEFYUvb"
   },
   "source": [
    "## Part 2: PCA\n",
    "We want to be able to distill the most important aspects, both for the purposes of visualization and de-noising. It may be the case that only the *d* most important axes of variation contribute significantly to our understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx4sx4PaYUvc"
   },
   "source": [
    "### 2.1: Plot the singular values of the matrix\n",
    "<font color=red>**In the cell below**</font>, create a chart comparing the singular values of the data matrix. Your x-axis should range from 1 to *m* (total dimension of the vector space), and your y-axis should plot the singular values in order from greatest to least. Feel free to use `np.linalg.svd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUviBKy7YUvd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppeArF5-YUvg"
   },
   "source": [
    "### 2.2 Choosing *d*:\n",
    "Based on the values you plotted, what should we set *d* to be for our purposes? Where's the cutoff in your judgement for the small set of useful features?\n",
    "\n",
    "<font color=red>**Fill out d below by replacing None with a value. It should be an integer. **</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0vv0iF8YUvi"
   },
   "outputs": [],
   "source": [
    "d = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tob7sWEYUvk"
   },
   "source": [
    "### 2.3: PCA Derivation/Motivation\n",
    "\n",
    "Let us assume that our data is drawn from a multivariate Gaussian distribution. Suppose we compute the singular value decomposition of our data matrix to be $X = U\\Sigma V^{\\intercal}$.\n",
    "\n",
    "1) What are the major properties of the matrices $U$, $V$ and $\\Sigma$? (Describe them like, $U$ is a \\_\\_\\_\\_ matrix)\n",
    "\n",
    "2) What is $X^\\intercal X$ in terms of $U$, $V$ and $\\Sigma$? Make sure you simplify based on the properties you described in part 1. As a hint, your answer should be a product of three terms.\n",
    "\n",
    "3) We know that $$\\frac{1}{n} X^\\intercal X$$ is the sample covariance matrix if we assume that each $X_i$ is drawn from a Gaussian distribution. This means it describes the joint variance of the features. Given this Gaussian, how may we construct a *d* dimensional basis to project our data? In other words, which vectors from $U$, $V$, etc do we take to be our projection basis? Hint: These vectors represent the directions with the greatest variance.\n",
    "\n",
    "(Note 1: we're trying to pick a vector basis with the highest variance because it will give us the most predictive power. Imagine trying to project to a basis where every data point gets projected to a similar or low-varying area. How are you going to classify anything? High variance ensures points are likely to be spread apart and thus more easily classified or clustered into distinct categories.)\n",
    "\n",
    "(Note 2: additionally, because our basis is orthonormal, we gain the advantage that the features we pick don't \"interfere\" with each other, and \"describe\" distinct aspects of the data.)\n",
    "\n",
    "<font color=red>**Answer in a text cell below, or scan and attach written answers to the submission pdf.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Emjj5fYUvl"
   },
   "source": [
    "### 2.4: Collect Basis Vectors\n",
    "Based on your answer to question 4, collect a basis of vectors that you will be projecting the data onto. The basis should be of length *d* (decided upon in section 2.2).\n",
    "\n",
    "<font color=red>**Replace None below with the correct basis.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5PzYZFnYUvm"
   },
   "outputs": [],
   "source": [
    "basis = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXeX8JSDYUvp"
   },
   "source": [
    "## Part 3: Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iREC8S3AYUvp"
   },
   "source": [
    "### 3.1: Projection function\n",
    "Please <font color=red>fill in the function below</font>, which projects a given vector and sends it to the space described by the input basis. Because the basis we're going to be using is orthonormal, we can use the formula described at https://home.apu.edu/~smccathern/PastCourses/F12/LinearAlgebra/LA5_1handout.pdf (Theorem 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkRQNJQmYUvq"
   },
   "outputs": [],
   "source": [
    "def project(v, basis):\n",
    "    \"\"\"Takes a vector v and projects it into the space spanned by basis.\n",
    "    \n",
    "    Args:\n",
    "        v: data vector (1-D numpy array)\n",
    "        basis: an _orthonormal_ basis of vectors\n",
    "        \n",
    "    Returns:\n",
    "        a 1-D numpy array, the projected vector\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_dKqd-PYUvs"
   },
   "source": [
    "### 3.2: Project & Plot\n",
    "We've chosen the dimension of our projection. We've chosen the vectors that will give us the most descriptive projection subspace. Now we just need to project our data and plot it.\n",
    "\n",
    "Please use the function in 3.1 and <font color=red>create a scatterplot of your projected data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXnCnBDpYUvt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDBt8mmOYUvu"
   },
   "source": [
    "## Part 4: Clustering\n",
    "Consider the scatterplot you've made. How many natural clusters do you think the data seems to form?\n",
    "\n",
    "<font color=red>Replace None below.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbvO904WYUvv"
   },
   "outputs": [],
   "source": [
    "k = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnAlQHUPYUv0"
   },
   "source": [
    "We're going to be employing the k-means algorithm, a form of unsupervised learning. The algorithm will produce an assignment of each data point to one of _k_ clusters (coloring your scatterplot *k* colors). A \"cluster\" is loosely defined by \"the collection of points gathered around a center point (centroid)\" The reason it's unsupervised is because we're not given any training data or labels, but we still have to infer a label for each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfS_jNMmYUv0"
   },
   "source": [
    "### 4.1: Understanding k-means Psuedocode\n",
    "**Variables**:\n",
    "\n",
    "$k$: the number of clusters\n",
    "\n",
    "$x_1, \\ldots, x_n$: the data (vectors)\n",
    "\n",
    "$l_1, \\ldots, l_n$: the corresponding label of each datapoint (scalars, value in interval \\[1, k\\])\n",
    "\n",
    "$c_1, \\ldots, c_k$: the centroids of each cluster (vectors)\n",
    "\n",
    "\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "> Initialize $c_1, \\ldots, c_n$ randomly.\n",
    ">\n",
    "> while (centroids still move more than $\\epsilon$ distance, for some small $\\epsilon > 0$)\n",
    ">\n",
    "> Assign each $l_i$ label for each point $x_i$ to the nearest centroid's index (if $c_3$ is the closest to $x_i$, set $l_i$ to be 3)\n",
    ">      \n",
    "> Update each centroid $c_i$ to be the average of all the $x_i$ points currently assigned to its cluster\n",
    "\n",
    "\n",
    "You may notice this functions kind of like the EM algorithm in that we update the centroid (hidden state) and our labels for the data in a loop based on each other. In fact, one might say that k-means is a non-probabilistic (\"hard\") version of EM specifically for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txQRe16OYUv1"
   },
   "source": [
    "### 4.2: Implement k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zzb6KLDYUv2"
   },
   "outputs": [],
   "source": [
    "def kmeans(xs, k):\n",
    "    \"\"\"Performs k-means clustering as described above.\n",
    "    \n",
    "    Args:\n",
    "        xs: list of vectors/data\n",
    "        k: the number of clusters to form\n",
    "    \n",
    "    Returns:\n",
    "        ls: list of corresponding labels for each data point, denoting which cluster each point belongs to\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # initialize centroids of clusters\n",
    "    cs = []  # list of centroids\n",
    "    for i in range(k):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    # labels\n",
    "    ls = [0 for i in range(len(xs))]\n",
    "    \n",
    "    # main loop goes here\n",
    "    while True:  # TODO: replace True condition with condition based on eps\n",
    "\n",
    "        # assign labels\n",
    "        for i in range(len(xs)):\n",
    "            # TODO: assign label\n",
    "            ls[i] = None  \n",
    "            \n",
    "        # recalculate centroids\n",
    "        for i in range(k):\n",
    "            # TODO: recalculate centroid position\n",
    "            cs[i] = None\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGdnwGfiYUv3"
   },
   "source": [
    "### 4.3 Plot projected data with color labels\n",
    "<font color=red>Below, use the 'c' argument in `plt.scatter` and the list of labels you derived to color the scatter plot you created in 3.2.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDC6cz7ZYUv4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
